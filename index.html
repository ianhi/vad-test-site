<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Silero VAD Test Page</title>
    <style>
        body { 
            font-family: Arial, sans-serif; 
            padding: 20px; 
            max-width: 800px; 
            margin: 0 auto; 
        }
        .version-section { 
            margin: 20px 0; 
            padding: 15px; 
            border: 2px solid #28a745; 
            border-radius: 8px; 
        }
        .result { 
            background: #f8f9fa; 
            padding: 8px; 
            margin: 5px 0; 
            font-family: monospace; 
            font-size: 12px;
            border-radius: 4px;
        }
        .success { background: #d4edda; }
        .error { background: #f8d7da; }
        .spectrogram-container { 
            height: 150px; 
            border: 1px solid #ccc; 
            margin: 10px 0;
            position: relative;
        }
        .vad-marker {
            position: absolute;
            top: 0;
            bottom: 0;
            border-left: 3px solid #ff0000;
            pointer-events: none;
            z-index: 10;
        }
        .vad-marker.start { border-left: 3px solid #00ff00; }
        .vad-region {
            position: absolute;
            top: 0;
            bottom: 0;
            background: rgba(255, 255, 0, 0.3);
            pointer-events: none;
            z-index: 5;
        }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 5px;
        }
        button:hover { background: #0056b3; }
        audio {
            width: 100%;
            margin: 10px 0;
        }
        .segments-table-container {
            max-height: 200px;
            overflow-y: auto;
            border: 1px solid #e0e0e0;
            border-radius: 4px;
            margin-top: 10px;
        }
        .segments-table {
            width: 100%;
            border-collapse: collapse;
        }
        .segments-table th, .segments-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .segments-table th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <h1>Silero VAD Test Page</h1>
    
    <p>This page allows you to test the Silero VAD (Voice Activity Detection) library in your browser.</p>
    <p>This project is open source! Please report any issues or contribute on <a href="https://github.com/ianhi/vad-test-site" target="_blank">GitHub</a>.</p>

    <div class="version-section">
        <p>This page is using version <strong>0.0.22</strong> of the <a href="https://docs.vad.ricky0123.com/" target="_blank">vad-web</a> library.</p>
    </div>
    
    <details id="settings-details" open style="margin: 20px 0; border: 1px solid #ccc; border-radius: 5px; padding: 10px;">
        <summary style="cursor: pointer; font-weight: bold;">‚öôÔ∏è VAD Settings</summary>
        <div id="vad-settings" style="display: grid; grid-template-columns: repeat(auto-fill, minmax(220px, 1fr)); gap: 15px; margin-top: 10px; padding-top: 10px; border-top: 1px solid #eee;">
            <div>
                <label for="positiveSpeechThreshold">Positive Speech Threshold</label>
                <input type="range" id="positiveSpeechThreshold" min="0" max="1" step="0.05" value="0.3">
                <span id="positiveSpeechThreshold-value">0.3</span>
            </div>
            <div>
                <label for="negativeSpeechThreshold">Negative Speech Threshold</label>
                <input type="range" id="negativeSpeechThreshold" min="0" max="1" step="0.05" value="0.2">
                <span id="negativeSpeechThreshold-value">0.2</span>
            </div>
            <div>
                <label for="redemptionFrames">Redemption Frames</label>
                <input type="number" id="redemptionFrames" value="32" style="width: 60px;">
            </div>
            <div>
                <label for="frameSamples">Frame Samples</label>
                <input type="number" id="frameSamples" value="512" style="width: 60px;">
            </div>
            <div>
                <label for="minSpeechFrames">Min Speech Frames</label>
                <input type="number" id="minSpeechFrames" value="3" style="width: 60px;">
            </div>
            <div>
                <label for="preSpeechPadFrames">Pre-speech Pad Frames</label>
                <input type="number" id="preSpeechPadFrames" value="4" style="width: 60px;">
            </div>
            <div>
                <label for="positiveSpeechPadFrames">Positive Speech Pad Frames</label>
                <input type="number" id="positiveSpeechPadFrames" value="4" style="width: 60px;">
            </div>
        </div>
    </details>
    
    <div style="text-align: center; margin: 20px 0; display: flex; flex-wrap: wrap; justify-content: center;">
        <button id="runTestButton" onclick="runTest()">üîÑ Run VAD Test</button>
        <input type="file" id="audio-file-input" accept="audio/*" onchange="loadLocalAudio(event)" style="display: none;">
        <button onclick="document.getElementById('audio-file-input').click()">üìÅ Load Audio File</button>
        <button id="recordBtn" onclick="toggleRecording()">üé§ Start Recording</button>
        <button onclick="downloadRecording()" id="downloadBtn" disabled>üíæ Download Recording</button>
    </div>
    
    <div id="recordingStatus" style="text-align: center; margin: 10px 0; display: none;">
        <span id="recordingText">üî¥ Recording...</span>
        <span id="recordingTime">0:00</span>
    </div>
    

    <div class="version-section">
        <h3>VAD Results</h3>
        <div id="spectrogram" class="spectrogram-container"></div>
        <div id="results"></div>
        <h3>Individual Segment Playback</h3>
        <div id="individual-segment-players"></div>
    </div>

    <h3>Individual Speech Segments</h3>
    <div id="segments-table-container" class="segments-table-container"></div>

    <h3>Raw Audio</h3>
    <audio id="audio-player" controls style="width: 100%; margin: 10px 0;"></audio>
    <h3>Full Speech Segment Playback</h3>
    <audio id="full-speech-player" controls style="width: 100%; margin: 10px 0;"></audio>

    <!-- Load ONNX runtime first -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.min.js"></script>
    <script>
        if (typeof ort !== 'undefined') ort.env.logLevel = 'error';
    </script>
    
    <!-- WaveSurfer for visualization -->
    <script src="https://unpkg.com/wavesurfer.js@7"></script>

    <script>
        let currentAudioBuffer = null;
        let currentAudioUrl = './patth.wav';
        let wavesurfer = null;
        let isTestRunning = false; // New flag to prevent concurrent runs
        
        // Recording variables
        let mediaRecorder = null;
        let recordedChunks = [];
        let isRecording = false;
        let recordingStartTime = 0;
        let recordingTimer = null;

        function addResult(containerId, text, type = '') {
            const div = document.createElement('div');
            div.className = `result ${type}`;
            div.textContent = text;
            document.getElementById(containerId).appendChild(div);
        }

        function clearResults() {
            document.getElementById('results').innerHTML = '';
        }

        function loadLocalAudio(event) {
            const file = event.target.files[0];
            if (file) {
                currentAudioUrl = URL.createObjectURL(file);
                addResult('results', `Loaded local file: ${file.name}`);
                runTest();
            }
        }

        async function loadAudio() {
            try {
                let blob;
                let audioUrl;
                
                if (currentAudioUrl.startsWith('blob:')) {
                    // Already a blob URL from recording
                    audioUrl = currentAudioUrl;
                    const response = await fetch(currentAudioUrl);
                    blob = await response.blob();
                } else {
                    // Regular file URL
                    const response = await fetch(currentAudioUrl);
                    blob = await response.blob();
                    audioUrl = URL.createObjectURL(blob);
                }
                
                document.getElementById('audio-player').src = audioUrl;
                
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }
                const arrayBuffer = await blob.arrayBuffer();
                currentAudioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                addResult('results', `‚úÖ Audio loaded: ${currentAudioBuffer.duration.toFixed(3)}s`, 'success');
                
                return { blob, audioUrl };
            } catch (error) {
                addResult('results', `‚ùå Audio load failed: ${error.message}`, 'error');
                throw error;
            }
        }

        async function createSpectrogram(containerId, audioUrl) {
            try {
                if (wavesurfer) {
                    wavesurfer.destroy();
                }

                const { default: WaveSurfer } = await import('https://unpkg.com/wavesurfer.js@7/dist/wavesurfer.esm.js');
                const { default: Spectrogram } = await import('https://unpkg.com/wavesurfer.js@7/dist/plugins/spectrogram.esm.js');
                
                wavesurfer = WaveSurfer.create({
                    container: `#${containerId}`,
                    waveColor: '#4F4A85',
                    progressColor: '#383351',
                    height: 150,
                    normalize: true
                });
                
                const spectrogramPlugin = Spectrogram.create({
                    container: `#${containerId}`,
                    labels: true,
                    height: 150
                });
                
                wavesurfer.registerPlugin(spectrogramPlugin);
                wavesurfer.load(audioUrl);
                
                return wavesurfer;
            } catch (error) {
                console.warn(`Spectrogram creation failed for ${containerId}:`, error);
                return null;
            }
        }

        async function runVAD(blob, resultsId, spectrogramId) {
            try {
                addResult(resultsId, `üîÑ Testing VAD...`);
                
                const script = document.createElement('script');
                script.src = `https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.22/dist/bundle.min.js`;
                
                await new Promise((resolve, reject) => {
                    script.onload = resolve;
                    script.onerror = reject;
                    document.head.appendChild(script);
                });
                
                let retries = 0;
                while (!window.vad && retries < 50) {
                    await new Promise(resolve => setTimeout(resolve, 100));
                    retries++;
                }
                
                if (!window.vad) {
                    throw new Error('VAD library not loaded');
                }

                // Log audio data stats for debugging waveform differences
                const audioDataForStats = currentAudioBuffer.getChannelData(0);
                const min = Math.min(...audioDataForStats);
                const max = Math.max(...audioDataForStats);
                const rms = Math.sqrt(audioDataForStats.reduce((sum, val) => sum + val * val, 0) / audioDataForStats.length);
                addResult(resultsId, `üìä Audio stats: min=${min.toFixed(4)}, max=${max.toFixed(4)}, rms=${rms.toFixed(4)}`);
                
                const vadInstance = await window.vad.NonRealTimeVAD.new({
                    positiveSpeechThreshold: parseFloat(document.getElementById('positiveSpeechThreshold').value),
                    negativeSpeechThreshold: parseFloat(document.getElementById('negativeSpeechThreshold').value),
                    redemptionFrames: parseInt(document.getElementById('redemptionFrames').value, 10),
                    frameSamples: parseInt(document.getElementById('frameSamples').value, 10),
                    minSpeechFrames: parseInt(document.getElementById('minSpeechFrames').value, 10),
                    preSpeechPadFrames: parseInt(document.getElementById('preSpeechPadFrames').value, 10),
                    positiveSpeechPadFrames: parseInt(document.getElementById('positiveSpeechPadFrames').value, 10)
                });
                
                const audioData = currentAudioBuffer.getChannelData(0);
                const sampleRate = currentAudioBuffer.sampleRate;
                
                const segments = [];
                for await (const { audio, start, end } of vadInstance.run(audioData, sampleRate)) {
                    segments.push({
                        startTime: start / 1000,
                        endTime: end / 1000
                    });
                }
                
                addResult(resultsId, `‚úÖ VAD: Found ${segments.length} segments`, segments.length > 0 ? 'success' : 'error');
                
                // Play full speech segment
                if (segments.length > 0) {
                    const fullSpeechAudio = new Float32Array(currentAudioBuffer.length);
                    let offset = 0;
                    segments.forEach(seg => {
                        const startSample = Math.floor(seg.startTime * currentAudioBuffer.sampleRate);
                        const endSample = Math.ceil(seg.endTime * currentAudioBuffer.sampleRate);
                        const segmentData = currentAudioBuffer.getChannelData(0).slice(startSample, endSample);
                        fullSpeechAudio.set(segmentData, offset);
                        offset += segmentData.length;
                    });

                    const fullSpeechBlob = new Blob([window.vad.utils.encodeWAV(fullSpeechAudio, currentAudioBuffer.sampleRate)], { type: 'audio/wav' });
                    const fullSpeechUrl = URL.createObjectURL(fullSpeechBlob);
                    document.getElementById('full-speech-player').src = fullSpeechUrl;
                }

                // Display segments in a table
                const segmentsTableContainer = document.getElementById('segments-table-container');
                segmentsTableContainer.innerHTML = ''; // Clear previous table

                const individualSegmentPlayersContainer = document.getElementById('individual-segment-players');
                individualSegmentPlayersContainer.innerHTML = ''; // Clear previous players

                if (segments.length > 0) {
                    const table = document.createElement('table');
                    table.className = 'segments-table';
                    table.innerHTML = `
                        <thead>
                            <tr>
                                <th>#</th>
                                <th>Start (s)</th>
                                <th>End (s)</th>
                                <th>Duration (s)</th>
                            </tr>
                        </thead>
                        <tbody>
                        </tbody>
                    `;
                    const tbody = table.querySelector('tbody');

                    segments.forEach((seg, i) => {
                        const row = tbody.insertRow();
                        const startSample = Math.floor(seg.startTime * currentAudioBuffer.sampleRate);
                        const endSample = Math.ceil(seg.endTime * currentAudioBuffer.sampleRate);
                        const segmentAudioData = currentAudioBuffer.getChannelData(0).slice(startSample, endSample);
                        
                        const segmentBlob = new Blob([encodeWAVBlob(segmentAudioData, currentAudioBuffer.sampleRate)], { type: 'audio/wav' });
                        const segmentUrl = URL.createObjectURL(segmentBlob);

                        row.innerHTML = `
                            <td>${i + 1}</td>
                            <td>${seg.startTime.toFixed(3)}</td>
                            <td>${seg.endTime.toFixed(3)}</td>
                            <td>${(seg.endTime - seg.startTime).toFixed(3)}</td>
                        `;
                        
                        const playerDiv = document.createElement('div');
                        playerDiv.innerHTML = `
                            <p>Segment ${i + 1}: ${seg.startTime.toFixed(3)}s - ${seg.endTime.toFixed(3)}s</p>
                            <audio controls src="${segmentUrl}"></audio>
                        `;
                        individualSegmentPlayersContainer.appendChild(playerDiv);
                    });
                    segmentsTableContainer.appendChild(table);
                } else {
                    segmentsTableContainer.innerHTML = '<p>No speech segments detected.</p>';
                    individualSegmentPlayersContainer.innerHTML = '';
                }
                
                segments.forEach((seg, i) => {
                    addResult(resultsId, `  Segment ${i + 1}: ${seg.startTime.toFixed(3)}s - ${seg.endTime.toFixed(3)}s`);
                });
                
                drawVADMarkers(spectrogramId, segments);
                
                document.head.removeChild(script);
                delete window.vad;
                
                return segments;
                
            } catch (error) {
                addResult(resultsId, `‚ùå VAD failed: ${error.message}`, 'error');
                return [];
            }
        }

        function drawVADMarkers(containerId, segments) {
            if (!currentAudioBuffer || segments.length === 0) return;
            
            const container = document.getElementById(containerId);
            const duration = currentAudioBuffer.duration;
            
            container.querySelectorAll('.vad-marker, .vad-region').forEach(el => el.remove());
            
            segments.forEach((segment, index) => {
                const startPercent = (segment.startTime / duration) * 100;
                const endPercent = (segment.endTime / duration) * 100;
                const widthPercent = endPercent - startPercent;
                
                const region = document.createElement('div');
                region.className = 'vad-region';
                region.style.left = startPercent + '%';
                region.style.width = widthPercent + '%';
                region.title = `Segment ${index + 1}: ${segment.startTime.toFixed(3)}s - ${segment.endTime.toFixed(3)}s`;
                container.appendChild(region);
                
                const startMarker = document.createElement('div');
                startMarker.className = 'vad-marker start';
                startMarker.style.left = startPercent + '%';
                container.appendChild(startMarker);
                
                const endMarker = document.createElement('div');
                endMarker.className = 'vad-marker';
                endMarker.style.left = endPercent + '%';
                container.appendChild(endMarker);
            });
        }

        async function toggleRecording() {
            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                recordedChunks = [];
                mediaRecorder = new MediaRecorder(stream);
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        recordedChunks.push(event.data);
                    }
                };
                
                mediaRecorder.onstop = () => {
                    const blob = new Blob(recordedChunks, { type: 'audio/wav' });
                    const url = URL.createObjectURL(blob);
                    currentAudioUrl = url;
                    document.getElementById('audio-player').src = url;
                    
                    
                    
                    // Update button text
                    document.getElementById('downloadBtn').disabled = false;
                    document.getElementById('recordBtn').textContent = 'üé§ Start Recording';
                    
                    // Stop all tracks
                    stream.getTracks().forEach(track => track.stop());

                    // Run VAD automatically after recording
                    runTest();
                };
                
                mediaRecorder.start();
                isRecording = true;
                recordingStartTime = Date.now();
                
                // Update UI
                document.getElementById('recordBtn').textContent = '‚èπÔ∏è Stop Recording';
                document.getElementById('downloadBtn').disabled = true;
                document.getElementById('recordingStatus').style.display = 'block';
                
                // Start timer
                updateRecordingTimer();
                recordingTimer = setInterval(updateRecordingTimer, 1000);
                
            } catch (error) {
                alert('Error accessing microphone: ' + error.message);
            }
        }

        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;
                
                // Clear timer
                if (recordingTimer) {
                    clearInterval(recordingTimer);
                    recordingTimer = null;
                }
                
                // Hide recording status
                document.getElementById('recordingStatus').style.display = 'none';
            }
        }

        function updateRecordingTimer() {
            const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
            const minutes = Math.floor(elapsed / 60);
            const seconds = elapsed % 60;
            document.getElementById('recordingTime').textContent = 
                `${minutes}:${seconds.toString().padStart(2, '0')}`;
        }

        function downloadRecording() {
            if (recordedChunks.length > 0) {
                const blob = new Blob(recordedChunks, { type: 'audio/wav' });
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = 'recording_' + new Date().getTime() + '.wav';
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            }
            document.getElementById('downloadBtn').disabled = true;
        }

        // Custom WAV encoder for better compatibility
        function encodeWAVBlob(samples, sampleRate) {
            const buffer = new ArrayBuffer(44 + samples.length * 2); // 44 bytes for WAV header, 2 bytes per sample
            const view = new DataView(buffer);

            // RIFF chunk descriptor
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + samples.length * 2, true); // file size
            writeString(view, 8, 'WAVE');

            // FMT sub-chunk
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true); // sub-chunk size
            view.setUint16(20, 1, true); // audio format (1 = PCM)
            view.setUint16(22, 1, true); // num channels
            view.setUint32(24, sampleRate, true); // sample rate
            view.setUint32(28, sampleRate * 1 * 2, true); // byte rate
            view.setUint16(32, 1 * 2, true); // block align
            view.setUint16(34, 16, true); // bits per sample

            // data sub-chunk
            writeString(view, 36, 'data');
            view.setUint32(40, samples.length * 2, true); // sub-chunk size

            // Write samples
            floatTo16BitPCM(view, 44, samples);

            return buffer;
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        function floatTo16BitPCM(output, offset, input) {
            for (let i = 0; i < input.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, input[i]));
                output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
        }

        async function runTest() {
            if (isTestRunning) {
                console.log("VAD test already running. Ignoring request.");
                return;
            }

            isTestRunning = true;
            document.getElementById('runTestButton').disabled = true; // Disable button

            clearResults();
            
            try {
                const { blob, audioUrl } = await loadAudio();
                
                await createSpectrogram('spectrogram', audioUrl);
                
                await runVAD(blob, 'results', 'spectrogram');
                
            } catch (error) {
                addResult('results', `‚ùå Test failed: ${error.message}`, 'error');
            } finally {
                isTestRunning = false;
                document.getElementById('runTestButton').disabled = false; // Enable button
            }
        }

        window.runTest = runTest;
        window.loadLocalAudio = loadLocalAudio;
        window.toggleRecording = toggleRecording;
        window.downloadRecording = downloadRecording;

        document.addEventListener('DOMContentLoaded', () => {
            const positiveSpeechThreshold = document.getElementById('positiveSpeechThreshold');
            const negativeSpeechThreshold = document.getElementById('negativeSpeechThreshold');
            const positiveSpeechThresholdValue = document.getElementById('positiveSpeechThreshold-value');
            const negativeSpeechThresholdValue = document.getElementById('negativeSpeechThreshold-value');

            positiveSpeechThreshold.addEventListener('input', (event) => {
                positiveSpeechThresholdValue.textContent = event.target.value;
            });

            negativeSpeechThreshold.addEventListener('input', (event) => {
                negativeSpeechThresholdValue.textContent = event.target.value;
            });

            // Run VAD on first user interaction
            const runOnFirstInteraction = () => {
                runTest();
                document.removeEventListener('mousedown', runOnFirstInteraction);
                document.removeEventListener('keydown', runOnFirstInteraction);
                document.removeEventListener('touchstart', runOnFirstInteraction);
            };

            document.addEventListener('mousedown', runOnFirstInteraction);
            document.addEventListener('keydown', runOnFirstInteraction);
            document.addEventListener('touchstart', runOnFirstInteraction);
        });
    </script>
    </h3>
</body>
</html>
